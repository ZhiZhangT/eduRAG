{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get JSON of an exam paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Load the PDF file\n",
    "pdf_path = \"exam_papers/bpghs_emath_prelim_paper1.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-4o', api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract JSON data from the response\n",
    "def extract_json_from_response(response):\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "        return data\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON from the response using regex\n",
    "        pattern = r'\\[.*\\]'\n",
    "        match = re.search(pattern, response, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                return data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error parsing JSON:\", e)\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No JSON data found in the response.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get meta JSON for document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt to extract meta information\n",
    "meta_info_prompt_template = '''\n",
    "You are provided with text from the first page of a PDF document. Extract the following meta information:\n",
    "- Subject (\"additional_mathematics\" or \"elementary_mathematics\")\n",
    "- School\n",
    "- Level (\"o_level\" or \"a_level\")\n",
    "- Year\n",
    "- Exam type (\"preliminary_exam\", \"final_exam\", \"mid_year_exam\")\n",
    "- Paper (1 or 2)\n",
    "\n",
    "Please output the result as a JSON object with these fields:\n",
    "\n",
    "```json\n",
    "[\n",
    "  <curly_bracket_start>\n",
    "  \"subject\": \"...\",\n",
    "  \"school\": \"...\",\n",
    "  \"level\": \"...\",\n",
    "  \"year\": \"...\",\n",
    "  \"exam_type\": \"...\",\n",
    "  \"paper\": \"...\"\n",
    "  <curly_bracket_end>\n",
    "]\n",
    "```\n",
    "\n",
    "Here is the text content:\n",
    "\n",
    "{text_content}\n",
    "'''\n",
    "\n",
    "meta_info_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_content\"],\n",
    "    template=meta_info_prompt_template,\n",
    ")\n",
    "\n",
    "meta_chain = LLMChain(llm=llm, prompt=meta_info_prompt)\n",
    "\n",
    "# Extract meta information from the first page\n",
    "first_page_content = documents[0].page_content\n",
    "meta_response = meta_chain.run(text_content=first_page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = extract_json_from_response(meta_response)\n",
    "meta_info = meta_info[0] if meta_info else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get JSON for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to match marks in square brackets (e.g., [4])\n",
    "marks_pattern = r'\\[(\\d+)\\]'\n",
    "\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_pages = []\n",
    "marks_per_page = {}\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    page_num = i + 1\n",
    "    page_content = doc.page_content\n",
    "\n",
    "    # Extract marks from the page using the regex\n",
    "    marks = re.findall(marks_pattern, page_content)\n",
    "    marks_per_page[page_num] = [int(mark) for mark in marks]  # Store marks as integers\n",
    "\n",
    "    current_chunk.append(f\"\\n\\n--- Page {page_num} ---\\n\\n{page_content}\")\n",
    "    current_pages.append(page_num)\n",
    "    if len(current_chunk) == max_pages_per_chunk or i == len(documents) - 1:\n",
    "        text_content = ''.join(current_chunk)\n",
    "        chunks.append((text_content, current_pages.copy()))\n",
    "        current_chunk = []\n",
    "        current_pages = []\n",
    "\n",
    "# Now `marks_per_page` contains all extracted marks per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amath_topics.json') as f:\n",
    "    amath_topics = json.load(f)\n",
    "    print(amath_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emath_topics.json') as f:\n",
    "    emath_topics = json.load(f)\n",
    "    print(emath_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "question_prompt_template =  \"\"\"\n",
    "    You are provided with text extracted from a PDF exam paper. The text may include multiple math questions along with diagrams. Ignore any solutions or answers provided in the text.\n",
    "    \n",
    "    Please identify each question, determine the question_number, which is an integer representing the full question (eg: 1, 2, 3), along with the question_part, a string representing the question with any sub-parts that it belongs to (eg: \"1\", \"2a\", \"2b\", \"2ai\", \"2aii\", \"3iv\", \"3v\" etc.).\n",
    "    After reading each question, express it in LaTeX format so that it can be rendered correctly.\n",
    "    Please also determine its page boundaries (page_start and page_end), and categorize it into one of the following categories:\n",
    "        {categories}\n",
    "\n",
    "    Output your result as a list of JSON objects enclosed in a code block like this:\n",
    "\n",
    "    ```json\n",
    "    [\"question\": \"...\", \"question_number\": \"...\", \"question_part\": \"...\", \"page_start\": ..., \"page_end\": ..., \"category\": \"...\"]\n",
    "    ```\n",
    "    Please ensure the output is valid JSON.\n",
    "\n",
    "    Here is the text content:\n",
    "    \n",
    "    {text_content}\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text_content\", \"categories\"],\n",
    "    template=question_prompt_template,\n",
    ")\n",
    "\n",
    "questions_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(question):\n",
    "    \"\"\"\n",
    "    Calculate a dynamic score for a question based solely on:\n",
    "    - Marks assigned to the question\n",
    "    \"\"\"\n",
    "    # Base score\n",
    "    base_score = 1.0\n",
    "\n",
    "    # Marks-based heuristic: assume 'marks' is provided in the question dictionary\n",
    "    marks = question.get('marks', 1)  # Default to 1 mark if not specified\n",
    "    mark_score = min(marks / 2, 5)  # Normalize marks to a score, capping max effect at 5\n",
    "\n",
    "    # Combine base and mark score\n",
    "    final_score = base_score + (mark_score * 1.5)  # Marks have a strong influence\n",
    "    return round(final_score, 2)  # Limit to 2 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "subject = meta_info.get(\"subject\")\n",
    "if subject == \"additional_mathematics\":\n",
    "    topics = amath_topics\n",
    "else:\n",
    "    topics = emath_topics\n",
    "    \n",
    "categories = list(topics.keys())\n",
    "\n",
    "for idx, (text_content, pages) in enumerate(chunks):\n",
    "    print(f\"Processing chunk {idx+1}/{len(chunks)}, pages {pages[0]}-{pages[-1]}\")\n",
    "\n",
    "    response = questions_chain.run(text_content=text_content, categories=categories)\n",
    "\n",
    "    data = extract_json_from_response(response)\n",
    "\n",
    "    if data is not None:\n",
    "        for question in data:\n",
    "            question['score'] = calculate_score(question)  # Dynamic scoring function\n",
    "        all_data.extend(data)\n",
    "    else:\n",
    "        print(f\"Failed to extract data for chunk {idx+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template for assigning question types\n",
    "question_type_prompt_template = '''\n",
    "Given the following question and its relevant topics, assign the most appropriate question type from the list.\n",
    "\n",
    "Question: {question_text}\n",
    "\n",
    "Relevant Question Types:\n",
    "{question_types_list}\n",
    "\n",
    "Please return the most appropriate question type from the list above. If none of the question types are relevant, you can return \"Unknown\".\n",
    "Simply return the question type as a string, nothing else.\n",
    "'''\n",
    "\n",
    "question_type_prompt = PromptTemplate(\n",
    "    input_variables=[\"question_text\", \"question_types_list\"],\n",
    "    template=question_type_prompt_template,\n",
    ")\n",
    "\n",
    "topics_chain = LLMChain(llm=llm, prompt=question_type_prompt)\n",
    "\n",
    "# Assuming `all_data` contains the parsed questions and their page information\n",
    "for question in all_data:\n",
    "    page_start = question['page_start']  # Page where the question starts\n",
    "\n",
    "    # Get marks for the corresponding page\n",
    "    page_marks = marks_per_page.get(page_start, [])\n",
    "\n",
    "    # Assign the first available mark to the question, or default to 1\n",
    "    if page_marks:\n",
    "        question['marks'] = page_marks.pop(0)  # Use and remove the first mark\n",
    "    else:\n",
    "        question['marks'] = 1  # Default mark if no marks are found\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine into one output JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine meta information with extracted questions\n",
    "output_data = {\n",
    "    \"meta_info\": meta_info,\n",
    "    \"questions\": all_data\n",
    "}\n",
    "\n",
    "# Save output to a JSON file\n",
    "subject = meta_info.get(\"subject\")\n",
    "school = meta_info.get(\"school\")\n",
    "year = meta_info.get(\"year\")\n",
    "exam_type = meta_info.get(\"exam_type\")\n",
    "paper = meta_info.get(\"paper\")\n",
    "\n",
    "output_filename = f'{subject}_{school}_{year}_{exam_type}_paper{paper}.json'\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(\"Extraction complete. Data saved to \", output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split PDF into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pypdfium2 as pdfium\n",
    "\n",
    "# Load a document\n",
    "pdf = pdfium.PdfDocument(pdf_path)\n",
    "\n",
    "paper_imgs_dir = f\"exam_papers/{output_filename}/\"\n",
    "os.makedirs(paper_imgs_dir, exist_ok=True)\n",
    "\n",
    "# Loop over pages and render\n",
    "for i in range(len(pdf)):\n",
    "    page = pdf[i]\n",
    "    image = page.render(scale=4).to_pil()\n",
    "    image.save(f'{paper_imgs_dir}pg{i+1}.jpg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
